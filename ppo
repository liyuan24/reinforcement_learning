import gymnasium as gym
import ale_py
from data_utils import preprocess_observation_batch
from networks import Policy, Critic
import torch
import numpy as np

# reference: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo.py#L81
def make_env(env_id, idx, capture_video, run_name):
    def thunk():
        if capture_video and idx == 0:
            env = gym.make(env_id, render_mode="rgb_array")
            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
        else:
            env = gym.make(env_id)
        env = gym.wrappers.RecordEpisodeStatistics(env)
        return env

    return thunk

def main():
    # Create the environment with human rendering that will display the game in a window
    num_envs = 10
    run_name = "ppo"
    capture_video = False
    # env setup
    # set the autoreset mode as NEXT_STEP(default) explicitly so that the readers are aware of the behavior
    # for more details check https://farama.org/Vector-Autoreset-Mode 
    # and source code https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/vector/sync_vector_env.py
    envs = gym.vector.SyncVectorEnv(
        [make_env("ALE/Pong-v5", i, capture_video, run_name) for i in range(num_envs)],
    )
    device = "cuda" if torch.cuda.is_available() else "cpu"
    checkpoint_save_path = "checkpoints"
    
    # Initialize the policy network, action space is UP and DOWN
    policy = Policy(input_dim=6400, action_space=2).to(device)
    critic = Critic(input_dim=6400).to(device)
    log_probs = []
    rewards = []
    # training epochs per data collection
    epochs = 5
    batch_size = 100  # the batch size for each epoch
    gamma = 0.99  # discount factor
    gae_lambda = 0.98
    learning_rate = 1e-4
    clip_epsilon = 0.2 # the clip epsilon in the clipped objective function for the PPO
    
    entropy_coef = 0.01 # the coefficient for the entropy bonus loss
    value_coef = 0.5 # the coefficient for the value network loss
    
    best_mean_rewards = -np.inf
    
    optimizer = torch.optim.AdamW(policy.parameters(), lr=learning_rate, fused=True)
    optimizer.zero_grad(set_to_none=True)
    
    iterations = 500 # number of iterations to collect data and optimize the policy/value network
    rollout_steps = 500 # the number of steps to collect data
    
    observations = torch.zeros(rollout_steps, num_envs, 6400).to(device)
    actions = torch.zeros(rollout_steps, num_envs, dtype=torch.long).to(device)
    rewards = torch.zeros(rollout_steps, num_envs).to(device)
    log_probs = torch.zeros(rollout_steps, num_envs).to(device)
    dones = torch.zeros(rollout_steps, num_envs).to(device)
    values = torch.zeros(rollout_steps, num_envs).to(device)
    
    game_observation, info = envs.reset()
    for i in range(iterations):
        ob = torch.zeros(num_envs, 6400).to(device)
        prev_ob = torch.zeros(num_envs, 6400).to(device)
        done = torch.zeros(num_envs).to(device)
        total_rewards = np.zeros(num_envs)
        # collecting rollout data esepcially the reward signal and action taken
        for j in range(rollout_steps):
            # Preprocess the current observation
            ob = preprocess_observation_batch(game_observation)
            ob = torch.from_numpy(ob).float().to(device)
            # if previous state is in terminal state, 
            # we don't need to subtract it from the current state because they are in different games
            prev_done = torch.zeros(num_envs).to(device) if j == 0 else dones[j-1].detach().clone()
            # if the previous state is in terminal state, the current state is the initial state of the next game
            # so we don't need to subtract the previous observation from the current observation
            prev_ob[prev_done == 1] = 0
            x = ob - prev_ob
            # make a copy of the current state so that they are not sharing the same storage
            prev_ob = ob.clone().detach()
            observations[j] = x
            dones[j] = done
            # feed into the policy network and value network
            with torch.no_grad():
                probs = policy(x) # [num_envs, 2]
                value = critic(x).flatten() # [num_envs]
            values[j] = value
            # sample an action from the policy network
            # [num_envs, 2] -> [num_envs]
            action = torch.multinomial(probs, num_samples=1).view(-1)
            actions[j] = action
            log_prob = torch.log(probs[np.arange(num_envs), action]).to(device)
            log_probs[j] = log_prob
            # UP is 2 and DOWN is 5: https://ale.farama.org/env-spec/#1
            pong_action = torch.where(action == 0, torch.tensor(2), torch.tensor(5)).cpu().numpy()
            # game_observation is next state
            # reward is the reward of the current action
            # terminated or truncated is indicating whether the next state is terminal or truncated
            game_observation, reward, terminated, truncated, info = envs.step(pong_action)
            total_rewards += reward
            # [num_envs]
            rewards[j] = torch.from_numpy(reward).float().to(device)
            done = torch.from_numpy(np.logical_or(terminated, truncated)).to(device)
        mean_rewards = total_rewards.mean()
        print(f"Average total rewards for {i} iteration: {mean_rewards}")
        if mean_rewards > best_mean_rewards:
            best_mean_rewards = mean_rewards
            checkpoint = {
                "policy": policy.state_dict(),
                "critic": critic.state_dict(),
                "optimizer": optimizer.state_dict(),
            }
            torch.save(checkpoint, f"{checkpoint_save_path}/ppo_{i}.pth")
        
        # data collection is done, compute the advantage
        ob = preprocess_observation_batch(game_observation)
        ob = torch.from_numpy(ob).float().to(device)
        prev_ob[dones[rollout_steps-1] == 1] = 0
        x = ob - prev_ob
        with torch.no_grad():
            next_value = critic(x).flatten().view(1, -1)
        last_advantage = 0
        advantages = torch.zeros(rollout_steps, num_envs).to(device)
        for i in reversed(range(rollout_steps)):
            if i == rollout_steps - 1:
                # if next step is in done state, we don't need to consider the value of the next step
                delta = rewards[i] + gamma * (1 - done.float()) * next_value - values[i]
                advantages[i] = last_advantage = delta
            else:
                # if next step is not in done state, we need to consider the value of the next step
                delta = rewards[i] + gamma * (1 - dones[i+1].float()) * values[i+1] - values[i]
                advantages[i] = last_advantage = delta + gae_lambda * gamma * (1 - dones[i+1].float()) * last_advantage
        # returns is the estimate of the expected rewards at step t taken action a_t
        # it will be used in the L2 loss of the value function estimation 
        returns = advantages + values
        
        # next step is to sample from the collected data and optimize the policy and value network with mini-batch gradient descent
        # flatten the data
        b_dones = dones.view(-1)
        # only consider the data that is not in done state
        b_obs = observations.view(-1, 6400)[b_dones == 0]
        b_actions = actions.view(-1)[b_dones == 0]
        b_log_probs = log_probs.view(-1)[b_dones == 0]
        b_advantages = advantages.view(-1)[b_dones == 0]
        b_returns = returns.view(-1)[b_dones == 0]
        
        total_samples = len(b_obs)
        b_inds = np.arange(total_samples)
        for epoch in range(epochs):
            np.random.shuffle(b_inds)
            for b_start in range(0, total_samples, batch_size):
                inds = b_inds[b_start:b_start+batch_size]
                batch_obs = b_obs[inds]
                batch_actions = b_actions[inds]
                batch_old_log_probs = b_log_probs[inds]
                batch_advantages = b_advantages[inds]
                # normalize the advantages
                batch_advantages = (batch_advantages - batch_advantages.mean()) / (batch_advantages.std() + 1e-8)
                batch_returns = b_returns[inds]
                
                new_probs = policy(batch_obs)
                new_values = critic(batch_obs).flatten()
                new_log_probs = torch.log(new_probs[np.arange(len(batch_actions)), batch_actions])
                
                log_ratio = new_log_probs - batch_old_log_probs
                ratio = torch.exp(log_ratio)
                
                # equation (7) in https://arxiv.org/abs/1707.06347
                # since we use gradient descent instead of gradient ascent, we use negative sign and use max instead of min accordingly
                clipped_loss_1 = -torch.clamp(ratio, 1-clip_epsilon, 1+clip_epsilon) * batch_advantages
                clipped_loss_2 = -ratio * batch_advantages
                clipped_loss = torch.max(clipped_loss_1, clipped_loss_2).mean()
                
                # loss for value network
                v_loss = 0.5 * ((new_values - batch_returns) ** 2).mean()
                
                # entropy bonus loss
                entropy = -new_probs * torch.log(new_probs + 1e-8)
                entropy_loss = entropy.mean()
                
                # total loss, equation (9) in https://arxiv.org/abs/1707.06347
                loss = clipped_loss - entropy_coef * entropy_loss + value_coef * v_loss
                
                # update the policy network parameters
                optimizer.zero_grad(set_to_none=True)
                # compute the gradient
                loss.backward()
                # add gradient clip
                torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=1.0)
                torch.nn.utils.clip_grad_norm_(critic.parameters(), max_norm=1.0)
                # update the parameters
                optimizer.step()
    envs.close()


if __name__ == "__main__":
    main()